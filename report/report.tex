\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{comment}
\usepackage{amsmath}
\usepackage[table]{xcolor}
\usepackage{amssymb}
\usepackage{graphicx}
%\usepackage{amsthm}
\usepackage[standard]{ntheorem} 
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{float}
%\newtheorem{theorem}{Theorem}
\newtheorem{condition}{Condition}

\title{Data-Driven Generative Models for the Deformation of Industrial Geometries}
\author{Guglielmo Padula}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{document}
\maketitle

\section{Preface}
Our line of work consists in studying \textbf{generative models} for shape optimization of complex geometries with a large number of parameters; the objective is also to reduce the number of relevant geometrical parameters, for example for modeling naval hulls, and creating new artificial geometries similar to real data, as there are non-generative techniques for creating new real geometries (for example Free Form Deformation) but using them can be costly. Our data is artificially created data using divergence-free deformations on a real bulbus model provided by Fincantieri. We study mainly two class of models: Variational Autoencoders and Generative Adversarial Networks. With at a least one model for every type we arrive at a relative reconstruction error of $0.002$.

\section{Introduction}
In the last five years there has been an increasing interest in using Deep Neural Network based models for generative new samples of 3D objects. There are also been some interest in deformation of 3d objects with the preservation of some characteristics, for example volume howewer we are not aware of the usage of generative models in this framework. The advantage of volume preserviness is important for industry because it could me same quantity of material used. 
We try to merge this gap by creating some generative models able to do deformations and preserve volume at the same time. Advantages of this methods is that typical methods can be very costly (see for example volume preserving Free Form Deformation. We test two main classes of models: Autoencoders and Generative Adversarial Networks. Another advantages of generative models is that the number of relevant parameter is greatly reduced.

\section{Related Work}
\subsection{Volume preserving deformations}
The first paper that talked of volume preserving deformations was Hirota et al \cite{hirota} which developed a method for doing a variation of Free Form Deformation that preserved the volume of the deformed mesh with respect to the original.\\
This argument has furter been developed by Hahmann \cite{hahmann}.
Von funck \cite{vonfunck} et. all (2006) proposed a method for creating deformations using divergence free vector fields and so preserving the volume.\\
Eisemberg et al. \cite{eisemberg} extends this method for shape interpolation, using the eigenvectors of the Von Neumann-Laplace equation for generating deformations. 
\subsection{Generative models for 3D meshes}
In the last years there have been an increasing number of generative models for 3D mesh deformation.\\
Qtan \cite{qtan} develops a Variational Autoencoder in which input data is encoded in the RIMD representation, which is rotation invariant.\\
Ranjan\cite{ranjan} develops a Convolutional Mesh Autoencoder (CoMA) which is based on Chebyscev Spectral Convolution.\\
Rana Hanocka \cite{hanocka} develops some new original convolutional layers for mesh data, and it also proposes an autoencoder.\\
Yuan \cite{yuan} extend the work of both Hanocka and Ranjan.\\
Hahner \cite{hahner} develop an Autoencoder model for semiregular meshes with different sizes.\\
Cheng \cite{cheng} et. al develop a GAN model based on Berthelot (2017).\\

\section{General metodology: CVPFFD and the Custom Layers}


\subsection{Assumption}
The main assumption of all the paper is the the points of the mesh that we are going to deform are all in $K=\{(x,y,z)|x\ge 0, y \ge 0, z\ge 0\}$ and will belong to this set after the deformation, and that no triangles interior intersect $\partial K$.  
We we will apply the methods presented here to our mesh, we will restrict $K$ to $(0,1)\times [0,1]\times(0,1)$.



\subsection{Constrained Volume Preserving Free Form Deformation}
Let $D=[0,1]\times[0,1]\times[0,1]$, $n$ a scalar and $P_{ijk}=\left[\frac{i}{n},\frac{j}{n},\frac{k}{n}\right]$ (called control points), and let $b^{i}_{n}$ the basis of $n+1$ bernstein polynomials
$$b_{\nu, n}(x)=\left(\begin{array}{l}
n \\
\nu
\end{array}\right) x^\nu(1-x)^{n-\nu}$$\\
Then by the property of bernstein polynomials
$$\sum_{i=0}^{n}b_{i}^{n}(u)b_{j}^{n}(v)b_{k}^{n}(w)P_{ijk}=\left[u,v,w\right]$$.
So it is possible to define a map parametrized by $Q\in\mathbb{R}^{3(n+1)}$
$$T_{Q}(u,v,w)=\sum_{i=0}^{n}b_{i}^{n}(u)b_{j}^{n}(v)b_{k}^{n}(w)(P_{ijk}+Q_{ijk})$$

This map is continous and it is called Free Form Deformation map.
If in general our mesh domain is $K$ we can use a continous map $\phi: \mathbb{R}^{3} \rightarrow \mathbb{R}^{3}$ such that $\phi(K)=\bar{K}\subseteq D$ and do $\phi^{-1}\circ T_{Q} \circ \phi$ to deform our space. So a Free Form definition is parametrized by $\phi$ and $Q$. In the literature $K$ in surrounded by a bounding box and $\phi$ is simple a linear map. One of the main properties of Free Form Deformation is the possibility to choose where to deform the mesh by moving the appropriate control points. 

Now a reasonable general definition of volume preserving free form deformation is a triple $\phi$,$Q$, and $f: \mathbb{R}^{3(n+1)} \rightarrow \mathbb{R}^{3(n+1)}$ such that $$Vol \circ \phi^{-1}\circ T_{f(Q)} \circ \phi (X)=Vol(X)$$ where $X$ is our mesh. The problem of finding an FFD reduces to find an appropriate $f$ that satisfies the constraint above. 
Notice that:
\begin{itemize}
\item given $Q$ and $\phi$ more than one $f$ satisfies the constraint.
\item Not all the $f$ are desiderable, for example the map $f(Q)=0$ reduces the entire FFD to identity, an so preserves volume trivially. 
\end{itemize}
To find an appropriate $f$ let's start by finding an appropriate formula for the volume.
\begin{Theorem}\label{thm:vol}
Let $M$ a closed connected mesh with points $P=(x,y,z)$ and triangles $T$ parametrized by triple of indices ordered such that all the face normals point outwards the mesh and that $x\ge 0,y\ge 0,z\ge 0$, it holds
$$Vol(M)=\sum_{t\in T} \frac{(x_{t_{1}}+x_{t_{2}}+x_{t_{3}})}{6}\left|\begin{array}{cl}
\left(y_{t_{2}}-y_{t_{1}}\right) & \left(z_{t_{2}}-z_{t_{1}}\right) \\
\left(y_{t_{3}}-y_{t_{1}}\right) & \left(z_{t_{3}}-z_{t_{1}}\right)
\end{array}\right|= $$ 
$$\sum_{t\in T} \frac{(y_{t_{1}}+y_{t_{2}}+y_{t_{3}})}{6}\left|\begin{array}{cl}
\left(x_{t_{1}}-x_{t_{2}}\right) & \left(z_{t_{1}}-z_{t_{2}}\right) \\
\left(x_{t_{3}}-x_{t_{2}}\right) & \left(z_{z_{3}}-z_{t_{2}}\right)
\end{array}\right|=$$
$$\sum_{t\in T} \frac{(z_{t_{1}}+z_{t_{2}}+z_{t_{3}})}{6}\left|\begin{array}{cl}
\left(x_{t_{1}}-x_{t_{3}}\right) & \left(y_{t_{1}}-y_{t_{3}}\right) \\
\left(x_{t_{2}}-x_{t_{3}}\right) & \left(y_{z_{2}}-y_{t_{3}}\right)
\end{array}\right|$$
\end{Theorem}
From here we see that the volume is trilinear in the point coordinates, so it is possible to act in a linear way on one coordinate at a time to obtain volume preservation. This is exactly the approach of $\cite{hahmann}$, which we will briefly report here. The following theorem is the result of three subsequent optimization problems, all with unique solution. $P_{ijk}^{x}$ will indicate the $x$-coordinate of $P_{ijk}$ similar for $y$ and $z$. 
\begin{proof}
See $\cite{hahmann}$
\end{proof}\hfill\\
VPFFD has an exact formula, however it may not safisfy our general assumptions. So we will consider only a subset of all possible VFFD.
\begin{Definition}[Constrained Volume Preserving Free Form Deformation]
\hfill\newline
If a VFFD respects
\begin{itemize}
\item $ T_{L}(\bar{K})>=0$ 
\item $T_{L}(\bar{K}\cap \{(x,y,z)| z=0 \})\subseteq  \{(x,y,z)| z=0 \}$
\item $T_{L}(\bar{K}\cap \{(x,y,z)| y=0 \})\subseteq  \{(x,y,z)| y=0 \}$
\item $T_{L}(\bar{K}\cap \{(x,y,z)| x=0 \})\subseteq  \{(x,y,z)| x=0 \}$
\item $T_{L}(\bar{K}\cap \{(x,y,z)| x*y*z\neq 0 \})\subseteq  \{(x,y,z)| x*y*z\neq 0 \}$
\end{itemize}
then it will be called Constrained Volume Preserving Free Form Deformation.
\end{Definition}
By looking at the optimization problem in \cite{hahmann} it can be easily shown that 
\begin{Theorem}\label{thm:cvffd}
\hfill\newline
If $$Q\in \mathcal{S}=\{ Q_{0jk}^{x}=0,Q_{i0k}^{y}=0, Q_{ik0}^{z}=0\} \cap \{  (\frac{i}{l}+Q_{ijk}^{x})^{2}\ge \sum_{i=1}^{l}\sum_{j=0}^{m}\sum_{k=0}^{n} (Q_{ijk}^{x})^{2} \forall i=1...l \} $$ 
$$\cap \{(\frac{j}{m}+Q_{ijk}^{y})^{2}\ge \sum_{i=0}^{l}\sum_{j=1}^{m}\sum_{k=0}^{n} (Q_{ijk}^{y})^{2} \forall j=1...m \} \cap \{(\frac{k}{n}+Q_{ijk}^{z})^{2}\ge \sum_{i=0}^{l}\sum_{j=0}^{m}\sum_{k=1}^{n} (Q_{ijk}^{z})^{2} \forall k=1...n \} $$ 
then we have a CVFFD.
\end{Theorem}
\begin{Theorem}
The set $\mathcal{S}$ is convex.
\end{Theorem}
\begin{proof}
$\mathcal{S}$ is the intersection of hyperplanes and volume internal to paraboloids, which are convex sets.
\end{proof}
We remark that this is only a sufficient condition, empirically we saw that if we deform only the control points which are not on the boundary by small values then we still get a CVFDD. \\

\subsection{Custom Layers}
In all our generative models, we use two different custom layer in the sampling phase:
\begin{itemize}
\item A smoothing layer
\item A volume layer
\end{itemize}
\subsubsection{Smoothing Layer}
This is a layer that applies laplacian smoothing $$K_{i}=\frac{1}{|N_{i}|}\sum_{j \in N_{i}} K_{j}$$ where $N_{i}$ are the neighbours of index $i$. Note that while $K_{i}$. Note that the laplacian smoother is applied only to points that are not on the boundary. Howewer the points of the boundary are still neighbours, so they influence the layer even if they are constant. 

\subsubsection{Volume Layer}
If we modify $Q^{x}$ by $\delta^{x}$ by theorem $\ref{thm:vol}$ we get that volume deviation is  $$\delta Vol(M)=\sum_{t\in T} \frac{(\delta^{x}_{t_{1}}+\delta^{x}_{t_{2}}+\delta^{x}_{t_{3}})}{6}\left|\begin{array}{cl}
\left(y_{t_{2}}-y_{t_{1}}\right) & \left(z_{t_{2}}-z_{t_{1}}\right) \\
\left(y_{t_{3}}-y_{t_{1}}\right) & \left(z_{t_{3}}-z_{t_{1}}\right)
\end{array}\right|=\sum f_{i}(M)^{x}\delta_{i}^{x}$$ where $$f_{i}(M)^{x}=\sum_{T|t_{1}==i} -\left|\begin{array}{cl}
\left(y_{t_{2}}-y_{t_{1}}\right) & \left(z_{t_{2}}-z_{t_{1}}\right) \\
\left(y_{t_{3}}-y_{t_{1}}\right) & \left(z_{t_{3}}-z_{t_{1}}\right)
\end{array}\right|+\\ \sum_{T|t_{2}==i} \left|\begin{array}{cl}
\left(y_{t_{2}}-y_{t_{1}}\right) & \left(z_{t_{2}}-z_{t_{1}}\right) \\
\left(y_{t_{3}}-y_{t_{1}}\right) & \left(z_{t_{3}}-z_{t_{1}}\right)
\end{array}\right|+ \\  \sum_{T|t_{3}==i} \left|\begin{array}{cl}
\left(y_{t_{2}}-y_{t_{1}}\right) & \left(z_{t_{2}}-z_{t_{1}}\right) \\
\left(y_{t_{3}}-y_{t_{1}}\right) & \left(z_{t_{3}}-z_{t_{1}}\right)
\end{array}\right|.$$ Same calculations holds for $y$ and $z$.
With this in mind we can write the following theorems

\begin{Theorem}
For every closed mesh $M$ $f_i(M)$ has a variable sign.
\end{Theorem}
\begin{proof}
Volume in invariant under translation so it must hold $\sum f_{i}^{x}=0$.
\end{proof}

\begin{Theorem}\label{thm:opt}
The optimization problem
$$\min_{\delta^{x}} \sum_{i} (\delta_{i}^{x})^{2}$$ \\
s.t  $$\begin{cases}\sum f_{i}^{x}\delta_{i}^{x}=a \\
\delta_{i}^{x}\ge -x_{i}+\epsilon \end{cases}$$, has an unique solution.

\end{Theorem}
\begin{proof}
The problem is always feasible because the $f_{i}$ have no constant sign.
The objective function is stricly convex and the domain is convex, so the result is trivial.
\end{proof}


\begin{Theorem}[Volume normalization layer]\label{thm:vl}
Let $K$ and $G$ two meshes with all non-negative points.
Let $a=\frac{1}{3}(Vol(K)-Vol(G))$
Let $\delta^{x}$ the solution of $$\min_{\delta^{x}} \sum_{i} (\delta_{i}^{x})^{2}$$ \\
s.t  $$\begin{cases}\sum f_{i}(G)^{x} \delta_{i}^{x}=a \\
\delta_{i}^{x}>\ge K^{x}_{i}+\epsilon \end{cases}$$.\\
Let $H$ such that $\begin{cases} H_{i}^{x}=G_{i}^{x}+\delta_{i}^{x}\\
H_{i}^{y}=G_{i}^{y}\\
H_{i}^{z}=G_{i}^{z}\\
\end{cases}$\\
Let $\delta^{y}$ the solution of $$\min_{\delta^{y}} \sum_{i} (\delta_{i}^{y})^{2}$$ \\
s.t  $$\begin{cases}\sum f_{i}(H)^{y}\delta_{i}^{y}=a \\
\delta_{i}^{y}>\ge K^{y}_{i}+\epsilon \end{cases}$$\\
Let $I$ such that $\begin{cases} I_{i}^{x}=H_{i}^{x}\\
I_{i}^{y}=H_{i}^{y}+\delta_{i}^{y}\\
I_{i}^{z}=H_{i}^{z}\\
\end{cases}$\\
Let $\delta^{z}$ the solution of $$\min_{\delta^{y}} \sum_{i} (\delta_{i}^{y})^{2}$$ \\
s.t  $$\begin{cases}\sum f_{i}(I)^{y}\delta_{i}^{y}=a \\
\delta_{i}^{z}>\ge K^{z}_{i}+\epsilon \end{cases}$$\\
Let $L$ such that $\begin{cases} L_{i}^{x}=I_{i}^{x}\\
L_{i}^{y}=I_{i}^{y}\\
L_{i}^{z}=I_{i}^{z}+\delta_{i}^{z}\\
\end{cases}$\\
Then $Vol(I)=Vol(K)$. 
\end{Theorem}
\begin{proof}
We already proved in theorem \ref{thm:opt} that all the problems are feasibile and that they have an unique solution.
Now by construction $Vol(L)=Vol(K)+3*\frac{1}{3}(Vol(K)-Vol(L))=Vol(K)$.
\end{proof}


\section{The Data}
We use a dataset which obtained by deforming a naval hull of Fincantieri usin Constrained Volume Preserving Free Form Deformation.
We apply CVPFFD to deform only the bulb of a naval hull, leaving the rest unchanged: the boundary of the hull is composed by the set $\{x*y*z=0\}$. The additional constraint guarantee that there will not be collision between points of the bulb and the other points. To avoid boundary problems we also deform only the points which are not on the boundary.
	
\begin{comment}
\subsection{Generation}

\subsubsection{Divergence Free Deformations using Laplacian Eigenfunctions}
A approach using divergence-free transformation (approach of Eisemberg \cite{eisemberg}). In detail we consider the functions defined on $\Omega =[0,1]^{3}$ and $n \in \mathbb{N}$\\
$$\begin{cases}
\Delta \psi_{n}=\lambda \psi_{n} & \Omega^{\circ} \\
\psi_{k}=0 & \partial\Omega
\end{cases}$$	
It is well known that, as $\Omega$ is compact $\phi_{k}$ form a basis of $L^{2}(\Omega)$ and that if $$\psi_{n}(x,y,z)=sin(\pi i_{n}) sin(\pi j_{n} y)sin(\pi k_{n}z)$$
where $i_{n},j_{n},k_{n}\in\mathbb{N}$ then $\lambda_{n}=-\pi^{2}(i_{n}^{2}+j_{n}^{2}+k_{n}^{2})$
From $\psi_{n}$ we get that $v_{n}=\nabla \times \psi_{n}$ safisfies $\nabla \cdot v_{n}=0$,and also $$v_{n}^{(0)}=\begin{bmatrix} 0 \\ \partial_{z}\psi_{n} \\ -\partial_{y}\psi_{n}\end{bmatrix}$$, $$v_{n}^{(1)}=\begin{bmatrix} \partial_{z}\psi \\ 0 \\ -\partial_{x}\psi_{n} \end{bmatrix}$$,
$$v_{n}^{(3)}=\begin{bmatrix} -\partial_{y}\psi_{n} \\ \partial_{x}\psi_{n} \\ 0 \end{bmatrix}$$
Let $\lambda_{n}$ be sorted decreasically, then our base will be $W=\cup_{n \in \mathbb{N}}\{v_{n}^{(0)},v_{n}^{(1)},v_{n}^{(2)}\}$.
Then we will take $v$ as $v=\sum_{i=1}^{N}a_{i}w_{i}$ where $a_{i}\sim Normal(0,\lambda_{i}^{-\frac{3}{2}})$. The the deformation $x$ given initial shape $x_{0}$ will be the solution of $\begin{cases} \frac{dx}{dt}=v(x(t)) \\ x(0)=x_{0} \end{cases}$.
If we take as a preprocessing plane $z=0$ we get that these transformations are exact on the mesh (i.e there is no discretization error due to triangles interior partially intersecting the domain.)
\subsubsection{Divergence Free Deformation using Vector Fields}
Another approach based on the one of Von Funck $\cite{vonfunck}$.
It is based on the fact, that given two scalar fields on $\mathbb{R}^{3}$ $p(x,y,z)$ and $q(x,y,z)$ then $v=\nabla p \times \nabla q$ as null divergence.
In general we will use the following parametrization:
$$\alpha,\beta \in [0,2\pi)$$ 

$$b\in \mathbb{R}^{3}$$
$$c \in \mathbb{R}^{3}$$
$$a \in \mathbb{R}^{3}$$
$$u=\begin{bmatrix} sin(\alpha)cos(\beta) \\ cos(\alpha)cos(\beta)\\
sin(\beta)\end{bmatrix}$$
$$v=\begin{bmatrix} -cos(\alpha) \\ sin(\alpha) \\ 0\end{bmatrix}$$.
$$p_{1}(x)=u \cdot (x-b) $$
$$q_{1}(x)=w \cdot (x-b)$$
$$p_{2}=a \cdot (x-c)$$
$$q_		{2}=(a\times (x-c))\cdot (a\times (x-c))$$
$$v=\nabla p_{1}\times \nabla q_{1}+\nabla p_{2}\times \nabla q_{2}=u\times w+ a \times (-2a \times (a \times (x-cd)))$$

Note that $u$ and $v$ are orthonormal and so $p_{1}$ and $q_{1}$ represent a costant vector field, and $q_{2}$ and $p_{2}$ represent a linear rotation field.
In total this model has 11 parameters.
\subsubsection{Non divergence-free deformations with normalization}
A mesh tipically is composed of $N$ points $p_{1},...p_{M}$ and $M$ triangles $T_{1},...,T_{M}$ which we will interpret as triples of points. It can be proved if the mesh is closed it volume is $\sum_{i=1}^{M} \frac{|Det(p_{T_{i}})|}{6}$.
Let be $A$ the region of plane deformed using a deformation function $\phi$, and $M$ a mesh. In general $Vol(\phi(M))\neq Vol(M)$, howewer we can adopt the following trick.
Let $V=Vol(M)$ and $Q$ the points obtained by applyng $\phi$ to $M$.
Let's suppose that we rescale all the points deformed by $\phi$ with a parameter $k$ in $\mathbb{R}$. We obtain the following expression for the volume:
$$f(k)=\sum_{i=1}^{M} k^{|A\cap T_{i}|}\frac{|det(Q_{T_{i}})|}{6}$$. 
And the volume diffenrence is $$g(k)=f(k)-V=\sum_{i=1}^{M} k^{|A\cap T_{i}|}\frac{|det(Q_{T_{i}})|}{6}-V$$ which is a cubic equation with all positive coefficients, so 
$\lim \limits_{+\infty} g(k)=+\infty$ and also $g(0)=\sum_{|T_{i}\cap A|=0}\frac{|det(Q_{P_{i}})|}{6}-V<0$ so exists $k^{\star}$ such that $g(k^{\star})=0$. 
In this way after rescaling by $k^{\star}$ the transformation has preserved the volume.
For deforming our data we use Free Form Deformation followed by normalization.

\subsection{Postprocessing}
To smooth the deformed mesh we apply Laplacian Smoothing:
$$x_{i}=\frac{1}{|N_{i}|}\sum_{j \in N_{i}} x_{j}$$ where $N_{i}$ are the neighbours of index $i$. Then normalization is applied again to preserve the volume.
\end{comment}
\section{Generative models}
%For estimating the latent space size we used Manifold Adaptive Dimension Estimation \cite{made}, because it is compliant with the Manifold Hypotesis used in neural networks. We got a latent space of dimension $3$. 
The objective is to sample new 3d objects with quality comparable to the original ones. The new samples should also have the same volume.
For this reason an explicit volume normalization layer is used.
The generative models also use a pretrained PCA to increase stability and quality.
We also use an explicit smoothing layer to increase quality.
In the experiments reported for the latent space size we use the number of basis elements we used for the deformations (3), howewer also smaller sizes can be used, and indeed should be used if the number of basis elements is high ($>5$) because of the curse of dimensionality.  
For estimating the quality of the generated 3d objects we decided to use Reconstruction Error, note that this is resonable because the divergence-free deformation basis may potentially generate all possible divergence-free deformations and so our dataset represents all the space.
In all the models we decided to use a standard normal distribution for sample from the latent space.\\
All the models are trained with AdamW with gradient clipping.
Every hidden units of our models are composed of:
\begin{itemize}
\item A linear layer
\item A Normalization Layer (typically Batch Normalization)
\item An Activation Layer (tipically ReLU)
\item A Dropout Layer
\end{itemize}
$ $\\
General notations:
\begin{itemize}
\item $\mathbb{R^{N}}$ latent space with a distance $d_{N}$
\item $X$ Hilbert space of the Data with associated distance $d$
\item $Z\sim MultivariateNormal(0_{N},I_{N})$ 
\item $A_{\theta}:X \rightarrow \mathbb{R}^{N}$ injective depending on a paramenter $\theta$
\item $B_{\phi}:\mathbb{R}^{N} \rightarrow X$ depending on a paramenter $\phi$
\item $C_{\psi}:X \rightarrow \mathbb{R}$ depending on a parameter $\psi$
\item $D_{\lambda}: \mathbb{R}^{N} \rightarrow \mathbb{R}$
\item $g:\mathbb{R} \rightarrow \mathbb{R}$ increasing   
\item $L_{Y}(y)$ will be likehood function of a random variable $Y$.
\item $\mu_{\theta}= E[A_{\theta}(X)]$
\item $\sigma_{\theta}=Var[(A_{\theta}(X))]^{\frac{1}{2}}$
\item $\hat{Z}_{\theta}\sim MultivariateNormal(\mu_{\theta},\sigma_{\theta})$
\item $f:\mathbb{R^{N}} \rightarrow \mathbb{R^{N}}$ parametrized by $\beta$
\item $\hat{\xi}_{X,n}^{Distribution}$ the MLE parameters associated to a distribution
\end{itemize}


\subsection{Simple Autoencoder toy model}
As a toy model for pure benchmarking we first implemented a very simple Autoencoder \cite{auto}, composed of two parts: an Encoder which encodes the mesh in a latent space of dimension, and a Decoder that takes a point of the latent space and returns it to the data space. They are training together in such a way that the are one the inverse of the other. So with our notation:
$$\theta^{\star},\phi^{\star}=\argmin \limits_{\theta,\psi}d(X,B_{\phi}(A_{\theta}(X)))$$

\subsection{Simple WGAN toy model}
A Generative Adversarial Network \cite{gan} is a generative model composed of a Generator and a Discriminator. The Generator tries to generate fake data and fooling the Discriminator and the Discriminator tries to discriminate between real and fake data. So with our notation:
$$\psi=\argmax_{\psi}[g(C_{\psi}(X))-g(C_{\psi}(B_{\phi}(Z_{p})))]$$
$$\phi=\argmax_{\phi} g(C_{\psi}(B_{\phi}(Z_{p})))$$
Note that this is a minmax game and so it is general hard to train.
For this reason we use a linear Discriminator as in the famous Wasserstain GAN paper \cite{wgan}.

\subsection{(simplified) Variational Autoencoder model}
A Variational Autoencoder \cite{varauto} is the Probabilistic version of the  AutoEncoder. The loss is usually approximated with the ELBO and the distribution are usually normal. Howewer as sampling with usual distribution does not guarantee volume preservation we use a deterministic decoder while keeping the probabilistic encoder.  So the reconstruction part of the loss is substitued with the our $d$ distance. We also use an hyperparameter $\alpha$ to improve the training (both of these techniques are also used in \cite{qtan}).
$$\theta^{\star},\phi^{\star},\beta^{\star}=\argmin_{\theta,\phi,\beta}[(1-\alpha)*d(X,B_{\phi}(A_{\theta}(X)))+\alpha* KL(\hat{Z}_{\theta},Z)]$$



\subsection{(simplified) Adversarial Autoencoder model}
An Adversarial Autoencoder \cite{aae} is similar to a VAE, it is regularized with an adversarial newtwork. We adopt network with the deterministic encoder. We use the same simplification adopted in the VAE. In our notation:
$$\alpha\in (0,1)$$
$$\theta^{\star},\phi^{\star}=\argmax\limits_{\theta,\phi}\alpha*(-d(X,B_{\phi}(A_{\theta}(X)))+(1-\alpha)*(E[g(C_{\psi}(\hat{Z}_{\theta}))])$$
$$\psi^{\star}=\argmax_{\lambda}  [E[g(D_{\lambda}(Z))]-E[g(D_{\lambda}(\hat{Z}_{\theta}))]]
$$


\subsection{BEGAN model}
A Boundary Equilibrium GAN \cite{began} is a GAN in which the Discriminator is an AutoEncoder. A simplified representation of its loss is:
$$\theta^{\star},\phi_{1}^{\star}=\argmin_{\theta,\phi_{1}}[ d(X,B_{\phi_{1}}(A_{\theta}(X)))-d(X,B_{\phi_{2}}(\hat{Z}_{\theta}))]$$
$$\phi_{2}^{\star}=\argmin d(X,B_{\phi_{2}}(Z))$$
\subsection{VAEWGAN model}
A VAEGAN \cite{vaegan} is similiar to an AAE model, howewer the discrinator acts on the space $X$, not in the latent space.
$$\gamma \in (0,1)$$
$$\mu_{\theta,\phi,\psi}=E[C_{\psi}(B_{\phi}(\hat{Z}_{\theta}))]$$
$$Y_{\theta,\phi,\psi}\sim MultivariateNormal(\mu_{\theta,\phi,\psi},I)$$
$$\theta^{\star}=\argmin\limits_{\theta} KL(\hat{Z}_{\theta},Z)-L_{Y_{\theta,\phi,\psi}}(C(X))$$
$$\phi^{\star}= \argmin\limits_{\phi} -\gamma*L_{Y_{\theta,\phi,\psi}}(C(X))+g(C_{\psi}(\hat{X}_{\theta,\phi}))+g(C_{\psi}(B_{\psi}(Z)))-g(C_{\psi}(X))$$
$$\psi^{\star}=\argmin\limits_{\psi}- g(C_{\psi}(\hat{X}_{\theta,\phi}))-g(C_{\psi}(B_{\psi}(Z)))+g(C_{\psi}(X))$$
We implement a VAEGAN with some adjustment taken from the WGAN paper\cite{wgan} ( linear discriminator), this obtaining a VAEWGAN.  

\subsection{Evaluation}
We use several metrics for evaluate our models.	\\
Non geometric metrics:
\begin{itemize}
\item The reconstruction error 
\item The variance of the generated meshes
\end{itemize}
Then we check some geometric properties of the real and the generated meshes using the relative MMD distance which
$$RelMMD(F)=\frac{\sqrt{E[||\xi(F(X))-\xi(F(B_{\phi}(Z)))||^{2}]}}{\sqrt{E[||\xi(F(X))||^{2}]}+\sqrt{E[||\xi(F(Y))||^{2}]}}$$ where $xi$ is the basis function of the RKHS associated to the laplacian kernel and F is the property that we want to compare. We measure the distance of:
\begin{itemize}
\item Gaussian discrete curvature
\item Total discrete curvature
\item Area
\end{itemize}
We also compare results with the ones of some meshes with more strong deformation (with negative curvare in some points) to check the effectivennes of our measures.

\subsection{Results}
\subsubsection{Hull}
Note that the variance of the data is $0.064$.
\begin{table}[H]
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
&  \cellcolor{green!30} AE & \cellcolor{green!30} AAE & \cellcolor{green!30} VAE & \cellcolor{green!30} BEGAN & \cellcolor{red!30} VAEWGAN & \cellcolor{red!30} WGAN  \\ \hline
$RelMMD(TC)$ & \cellcolor{green!30} 0.60 & \cellcolor{green!30} 0.57  & \cellcolor{green!30} 0.60 & \cellcolor{green!30}  0.558 & \cellcolor{red!30} a & \cellcolor{red!30} a \\ \hline
$RelMMD(Area)$  & \cellcolor{green!30} 0.03 & \cellcolor{green!30} 0.01 & \cellcolor{green!30} 0.003 & \cellcolor{green!30} 0.016 & \cellcolor{red!30} a &   \cellcolor{red!30} a \\ \hline
$E[\min\limits_{X} \frac{||B_{\phi}(Z)-X||}{||X||}]$& \cellcolor{green!30} 0.021 & \cellcolor{green!30} 0.015 & \cellcolor{green!30} 0.012 & \cellcolor{green!30} 0.019 & \cellcolor{red!30} a& \cellcolor{red!30} a  \\ \hline
$Var(B_{\phi}(Z))$& \cellcolor{green!30} 0.026 & \cellcolor{green!30}  0.03 & \cellcolor{green!30} 0.024 & \cellcolor{green!30} 0.068 &  \cellcolor{red!30} a & \cellcolor{red!30} a \\ \hline
$RelMMD(GC)$ & \cellcolor{green!30} 0.189 & \cellcolor{green!30} 0.10 & \cellcolor{green!30} 0.114 & \cellcolor{green!30} 0.14 & \cellcolor{red!30} a & \cellcolor{red!30} a \\ \hline
$RelMMD(Id)$ & \cellcolor{green!30} 0.013 & \cellcolor{green!30} 0.0079 & \cellcolor{green!30} 0.008 & \cellcolor{green!30} 0.01 & \cellcolor{red!30} a & \cellcolor{red!30} a\\ \hline
\end{tabular}
\end{table}Colomns in green indicate models that have reached convergence in the training phase. Red columns indicate problems that have not reached convergence while the blue coloumn represents a dataset from which we don't want to sample from because it has non phisically admissible configurations (negative gaussian curvature in some points).
The best model is the Adversarial Autoencoder, which has a slightly lesser variance then the other models, howewer it approximates best the distribution of the geometric properties.
\begin{comment}
\subsubsection{Rabbit}
\end{comment}




\begin{comment}
\section{Appendix}
\subsection{Proof of v expression in 4.0.2}
$\nabla p_{1}=u$ trivially.\\
$\nabla q_{1}=w$ trivially.
$\nabla p_{2}=a$ trivially
$$\nabla q_{2}=2 \nabla (a \times (x-c))\cdot (a \times (x-c))=$$ 
$$-2(a \times \nabla (x-c))\cdot(a\times (x-c))= $$ $$-2\begin{bmatrix} 0 & a_{3} & -a_{2} \\ -a_{3} & 0 & a_{1} \\ a_{2} & -a_{1} & 0 \end{bmatrix} (a \times (x-c))^{T} $$
$$=2 a\times \begin{bmatrix} 0 & a_{3} & -a_{2} \\ -a_{3} & 0 & a_{1} \\ a_{2} & -a_{1} & 0 \end{bmatrix}(x-c)^{T}=-2 a \times (a \times (x-c) ) $$
\end{comment}
		
\bibliographystyle{unsrt}
\bibliography{refs}




\end{document}
