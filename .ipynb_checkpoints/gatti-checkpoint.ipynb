{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4862ec7-2e5f-453e-bd39-40cf5e0b2848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyberguli/.conda/envs/sissa/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stl import mesh\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.examples.util  # patches torchvision\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4124b61-6b7b-4134-a031-75e9ead2d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_mesh0 = mesh.Mesh.from_file('orig.stl')\n",
    "temp0=your_mesh0.data[0][1]\n",
    "for i in range(1,3430):\n",
    "    temp0=np.concatenate((temp0,your_mesh0.data[i][1]), axis=0)\n",
    "Pointset0 = np.unique(temp0, axis=0)\n",
    "neigh0=[]\n",
    "N=len(Pointset0)\n",
    "for i in range(len(Pointset0)):\n",
    "    lst=[]\n",
    "    for j in range(3430):\n",
    "        if Pointset0[i].tolist() in your_mesh0.data[j][1].tolist():\n",
    "            for k in range(3):\n",
    "                if Pointset0[i].tolist()!=your_mesh0.data[j][1][k].tolist():\n",
    "                    lst.append(your_mesh0.data[j][1][k].tolist())\n",
    "    neigh0.append(lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e003686-6f7e-4f19-9d52-d795db9d270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_mesh1 = mesh.Mesh.from_file('front0.stl')\n",
    "temp1=your_mesh1.data[0][1]\n",
    "for i in range(1,3430):\n",
    "    temp1=np.concatenate((temp1,your_mesh1.data[i][1]), axis=0)\n",
    "Pointset1 = np.unique(temp1, axis=0)\n",
    "neigh1=[]\n",
    "N=len(Pointset1)\n",
    "for i in range(N):\n",
    "    lst=[]\n",
    "    for j in range(3430):\n",
    "        if Pointset1[i].tolist() in your_mesh1.data[j][1].tolist():\n",
    "            for k in range(3):\n",
    "                if Pointset1[i].tolist()!=your_mesh1.data[j][1][k].tolist():\n",
    "                    lst.append(your_mesh1.data[j][1][k].tolist())\n",
    "    neigh1.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1555c9c6-307c-4232-b187-fa4ffd539604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(x,y,z):\n",
    "         return np.arccos(np.sum((x-y)*(x-z)))/(np.linalg.norm(x-y)*np.linalg.norm(x-z))\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35b8aa-5738-4e2e-96ce-4761869c9790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a714902-407f-48ec-928d-7be94ef5584b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1341cc95-b687-48bf-ac3b-fc45364c90b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8506/1180019495.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  T[i,a,b]=temp0/temp1\n",
      "/tmp/ipykernel_8506/1180019495.py:22: RuntimeWarning: invalid value encountered in sqrt\n",
      "  D=np.diag(D)**0.5\n"
     ]
    }
   ],
   "source": [
    "T=np.zeros([N,3,3])\n",
    "for i in range(N):\n",
    "    for a in range(3):\n",
    "        for b in range(3):\n",
    "            temp0=0\n",
    "            temp1=0\n",
    "            for j in neigh[i]: \n",
    "                    p0=Pointset0[i]-Pointset0[j]\n",
    "                    p1=Pointset1[i]-Pointset1[j]\n",
    "                    temp0=temp0+C[i,j]*p0[b]*p1[a]\n",
    "                    temp1=temp1+C[i,j]*p0[b]*p0[b]\n",
    "    T[i,a,b]=temp0/temp1\n",
    "T=np.nan_to_num(T,nan=1)\n",
    "R=np.zeros([N,3,3])\n",
    "S=np.zeros([N,3,3])\n",
    "dR=np.zeros([N,N,3,3])\n",
    "temp=np.zeros([N,3,3])\n",
    "\n",
    "for i in range(N):\n",
    "    temp[i]=np.matmul(T[i].T,T[i])\n",
    "    D,Q=np.linalg.eigh(temp[i])\n",
    "    D=np.diag(D)**0.5\n",
    "    S[i]=np.matmul(Q.T,np.matmul(D,Q))\n",
    "    R[i]=np.matmul(T[i],np.linalg.inv(S[i]))\n",
    "\n",
    "    \n",
    "RIMDi0=S[0]\n",
    "for j in neigh[0]:\n",
    "    dR[0][j]=np.matmul(R[0].T,R[j])\n",
    "    RIMDi0=np.concatenate((dR[0,j],RIMDi0),axis=1)\n",
    "RIMD=RIMDi0\n",
    "for i in range(N):\n",
    "    RIMDi0=S[i]\n",
    "    for j in neigh[i]:\n",
    "        dR[i][j]=np.matmul(R[i].T,R[j])\n",
    "        RIMDi=np.concatenate((dR[i,j],RIMDi),axis=1)\n",
    "    RIMD=np.concatenate((RIMD,RIMDi),axis=1)\n",
    "RIMD=RIMD.reshape(3*70236)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ef031-6426-452f-a21a-1b461cc5450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRIMD(reference,deformed):\n",
    "    #gets point and implicit neighbouroods of reference\n",
    "    your_mesh0 = mesh.Mesh.from_file(reference)\n",
    "    temp0=your_mesh0.data[0][1]\n",
    "    for i in range(1,3430):\n",
    "        temp0=np.concatenate((temp0,your_mesh0.data[i][1]), axis=0)\n",
    "    Pointset0 = np.unique(temp0, axis=0)\n",
    "    neigh0=[]\n",
    "    N=len(Pointset0)\n",
    "    for i in range(len(Pointset0)):\n",
    "        lst=[]\n",
    "        for j in range(3430):\n",
    "            if Pointset0[i].tolist() in your_mesh0.data[j][1].tolist():\n",
    "                for k in range(3):\n",
    "                    if Pointset0[i].tolist()!=your_mesh0.data[j][1][k].tolist():\n",
    "                        lst.append(your_mesh0.data[j][1][k].tolist())\n",
    "        neigh0.append(lst)\n",
    "    #gets point and and neigh of deformed\n",
    "    your_mesh1 = mesh.Mesh.from_file(deformed)\n",
    "    temp1=your_mesh1.data[0][1]\n",
    "    for i in range(1,3430):\n",
    "        temp1=np.concatenate((temp1,your_mesh1.data[i][1]), axis=0)\n",
    "    Pointset1 = np.unique(temp1, axis=0)\n",
    "    neigh1=[]\n",
    "    for i in range(N):\n",
    "        lst=[]\n",
    "        for j in range(3430):\n",
    "            if Pointset1[i].tolist() in your_mesh1.data[j][1].tolist():\n",
    "                for k in range(3):\n",
    "                    if Pointset1[i].tolist()!=your_mesh1.data[j][1][k].tolist():\n",
    "                        lst.append(your_mesh1.data[j][1][k].tolist())\n",
    "        neigh1.append(lst)\n",
    "     #calculates the matrix of the angles\n",
    "    C=np.zeros([N,N])\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i!=j:\n",
    "                temp=intersection(neigh0[i],neigh0[j])\n",
    "                if len(temp)==2:            \n",
    "                    C[i,j]=abs(1/np.tan(calculate_angle(temp[0],Pointset0[i],Pointset0[j])))+abs(1/np.tan(calculate_angle(temp[1],Pointset0[i],Pointset0[j])))\n",
    "     #calculates real neigh\n",
    "    neigh=[1]*N\n",
    "    for i in range(N):\n",
    "        neigh[i]=[j for j in range(N) if C[i,j]!=0]\n",
    "    #calculates T\n",
    "    T=np.zeros([N,3,3])\n",
    "    for i in range(N):\n",
    "        for a in range(3):\n",
    "            for b in range(3):\n",
    "                temp0=0\n",
    "                temp1=0\n",
    "                for j in neigh[i]: \n",
    "                        p0=Pointset0[i]-Pointset0[j]\n",
    "                        p1=Pointset1[i]-Pointset1[j]\n",
    "                        temp0=temp0+C[i,j]*p0[b]*p1[a]\n",
    "                        temp1=temp1+C[i,j]*p0[b]*p0[b]\n",
    "                T[i,a,b]=temp0/temp1\n",
    "    T=np.nan_to_num(T,nan=1)\n",
    "    R=np.zeros([N,3,3])\n",
    "    S=np.zeros([N,3,3])\n",
    "    dR=np.zeros([N,N,3,3])\n",
    "    temp=np.zeros([N,3,3])\n",
    "    #calculates R and S\n",
    "    for i in range(N):\n",
    "        temp[i]=np.matmul(T[i].T,T[i])\n",
    "        D,Q=np.linalg.eigh(temp[i])\n",
    "        D=np.diag(D)**0.5\n",
    "        S[i]=np.matmul(Q.T,np.matmul(D,Q))\n",
    "        R[i]=np.matmul(T[i],np.linalg.inv(S[i]))\n",
    "\n",
    "    \n",
    "    RIMDi0=S[0]\n",
    "    for j in neigh[0]:\n",
    "        dR[0][j]=np.matmul(R[0].T,R[j])\n",
    "        RIMDi0=np.concatenate((dR[0,j],RIMDi0),axis=1)\n",
    "    RIMD=RIMDi0\n",
    "    for i in range(N):\n",
    "        RIMDi=S[i]\n",
    "        for j in neigh[i]:\n",
    "            dR[i][j]=np.matmul(R[i].T,R[j])\n",
    "            RIMDi=np.concatenate((dR[i,j],RIMDi),axis=1)\n",
    "        RIMD=np.concatenate((RIMD,RIMDi),axis=1)\n",
    "    RIMD=RIMD.reshape(3*70236)\n",
    "    return RIMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b39bfc-3752-4cde-92eb-b16f0b945871",
   "metadata": {},
   "outputs": [],
   "source": [
    "RIMD=createRIMD('orig.stl','front0.stl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d29c5-c385-414f-9003-a7231cefb4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # setup the two linear transformations used\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, 3*70236)\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        # define the forward computation on the latent z\n",
    "        # first compute the hidden units\n",
    "        hidden = self.softplus(self.fc1(z))\n",
    "        # return the parameter for the output Bernoulli\n",
    "        # each is of size batch_size x 784\n",
    "        loc_img = self.sigmoid(self.fc21(hidden))\n",
    "        return loc_img\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # setup the three linear transformations used\n",
    "        self.fc1 = nn.Linear(3*70236, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define the forward computation on the image x\n",
    "        # first shape the mini-batch to have pixels in the rightmost dimension\n",
    "        x = x.reshape(-1, 3*70236)\n",
    "        # then compute the hidden units\n",
    "        hidden = self.softplus(self.fc1(x))\n",
    "        # then return a mean vector and a (positive) square root covariance\n",
    "        # each of size batch_size x z_dim\n",
    "        z_loc = self.fc21(hidden)\n",
    "        z_scale = torch.exp(self.fc22(hidden))\n",
    "        return z_loc, z_scale\n",
    "\n",
    "        \n",
    "class VAE(nn.Module):\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, z_dim=50, hidden_dim=400, use_cuda=False):\n",
    "        super().__init__()\n",
    "        # create the encoder and decoder networks\n",
    "        self.encoder = Encoder(z_dim, hidden_dim)\n",
    "        self.decoder = Decoder(z_dim, hidden_dim)\n",
    "\n",
    "        if use_cuda:\n",
    "            # calling cuda() here will put all the parameters of\n",
    "            # the encoder and decoder networks into gpu memory\n",
    "            self.cuda()\n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    # define the model p(x|z)p(z)\n",
    "    def model(self, x):\n",
    "        # register PyTorch module `decoder` with Pyro\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # setup hyperparameters for prior p(z)\n",
    "            z_loc = x.new_zeros(torch.Size((x.shape[0], self.z_dim)))\n",
    "            z_scale = x.new_ones(torch.Size((x.shape[0], self.z_dim)))\n",
    "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
    "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "            # decode the latent code z\n",
    "            loc_img = self.decoder(z)\n",
    "            # score against actual images\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(loc_img).to_event(1), obs=x.reshape(-1, 3*70236))\n",
    "\n",
    "    # define the guide (i.e. variational distribution) q(z|x)\n",
    "    def guide(self, x):\n",
    "        # register PyTorch module `encoder` with Pyro\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # use the encoder to get the parameters used to define q(z|x)\n",
    "            z_loc, z_scale = self.encoder(x)\n",
    "            # sample the latent code z\n",
    "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "            \n",
    "    def fitted(self, x):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.encoder(x)\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        loc_img = self.decoder(z)\n",
    "        return loc_img\n",
    "\n",
    "def train(svi, train_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for x in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(x)\n",
    "        \n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train\n",
    "\n",
    "def evaluate(svi, test_loader, use_cuda=False):\n",
    "\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    for x in test_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(x)\n",
    "\n",
    "        hello=vae.fitted(x)\n",
    "        \n",
    "    return hello\n",
    "\n",
    "# Run options\n",
    "LEARNING_RATE = 1.0e-3\n",
    "USE_CUDA = False\n",
    "# Run only for a single iteration for testing\n",
    "NUM_EPOCHS = 1 if smoke_test else 10\n",
    "TEST_FREQUENCY = 10\n",
    "\n",
    "train_loader=[]\n",
    "train_loader.append(RIMD)\n",
    "\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# setup the VAE\n",
    "vae = VAE(use_cuda=USE_CUDA)\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": LEARNING_RATE}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []\n",
    "# training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "'''\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        # report test diagnostics\n",
    "        total_epoch_loss_test = evaluate(svi, test_loader, use_cuda=USE_CUDA)\n",
    "        test_elbo.append(-total_epoch_loss_test)\n",
    "        print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a652a8a8-d8eb-4795-83e4-35dc2d18a20c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
